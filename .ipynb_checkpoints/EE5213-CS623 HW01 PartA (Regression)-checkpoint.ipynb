{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE5213/CS623 Assignment 1: Part A (Predicting house prices using Regression)\n",
    "\n",
    "Before we start, please put your name and LUMS ID(8 digit) in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollnumber = 21100171\n",
    "name = \"Muhammad Sameer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions (IMPORTANT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When completing the notebook, please adhere to the following rules:\n",
    "- Do not write or modify any code outside of code blocks. (You can make copy of the notebook if you want to play with the code, but for the notebook you submit the code should remain un-modified).\n",
    "\n",
    "```python\n",
    "########### WRITE YOU CODE BELOW ######\n",
    "\n",
    "print(\"Hello World\")\n",
    "\n",
    "########### END OF CODE ###############\n",
    "```\n",
    "\n",
    "- Do not add or delete any cells from the notebook. You may add new cells to perform scatch work, but delete them before submitting.\n",
    "- Run all cells before submitting. You will only get credit for code that has been run (for the output that has been shown in this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression - Predicting House Price\n",
    "\n",
    "In the first section of this assignment, we will train a linear regression for the housing pricing example covered in class. Our goal is to predict the price of a house from four input features: House Area, Number of Bedrooms, Years Built and Median Inocome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Lets start by creating NumPy arrays to store the housing data from slide 54 of lecture 2 notes.\n",
    "\n",
    "Note that we have changed slightly the units used for the data. For example in the lecture notes the house area is given in sq ft, whereas here we specify Area in thousands of Sq ft. So the area of a 3500 Sqt Ft house is represented here as 3.5. Similar change has been done for other variables. This is done to bring all inputs in a similar numeric range, otherwise inputs with large values such as median income would have a disproportionately high contibution to the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our training data has 7 input samples and corresponding outputs\n",
    "X = np.array([  [3.5,  4.2,  3.3,  3.0,  2.0, 4.5,  3.0],\n",
    "                [3,    4,    2,    3,    1,   3,    2],\n",
    "                [3.7,  2.5,  1.1,  1.9,  0.5, 5.0,  4.0],\n",
    "                [1.0,  1.0,  0.75, 0.75, 1,   0.75, 1.0]])\n",
    "\n",
    "y = np.array([1.2, 1.5, 0.9, 0.85, 0.82, 1.1, 0.93])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function (Graded)\n",
    "\n",
    "Recall that we can compute the predicted output y' for a single input feature vector $\\vec{x}$ as: \n",
    "\n",
    "- y' = $\\vec{w}$.$\\vec{x}$ + b\n",
    "\n",
    "And the predicted output vector $\\vec{y'}$ for all m outputs can be calculated from input matrix <b>X</b> as:\n",
    "\n",
    "- $\\vec{y'}$ = $\\vec{w}$.<b>X</b> + b\n",
    "\n",
    "The loss function $L_{mse}$ can then be calculated from observed output vector $\\vec{y}$ and predicted output vector $\\vec{y'}$ as:\n",
    "\n",
    "\\begin{equation}\n",
    " L = \\frac{1}{m} \\sum_{i=1}^{m} (y'^{(i)}-y^{(i)})^2\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "You will now implement the MSE loss function:\n",
    "\n",
    "- Inputs to the function are the weight vetor $\\vec{w}$, bias b, input matrix <b>X</b> and output fecture vector $\\vec{y}$\n",
    "- Add your code to first calculate pedicted output y_prime and then caluclate loss from y and y_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(w, b, X, y):\n",
    "    \n",
    "    # number of training instances\n",
    "    m = X.shape[1]\n",
    "\n",
    "    ########### WRITE YOU CODE BELOW ######\n",
    "    \n",
    "    loss = 0 \n",
    "    loss = (1/m)*(np.sum(np.square(y - ( (w@X) + b) )))\n",
    "    \n",
    "    \n",
    "    ########### END OF CODE ###############\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now test the output of your cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your loss value:\n",
      "0.048882142857142805\n",
      "\n",
      "Correct loss value:\n",
      "0.04888214285714284\n"
     ]
    }
   ],
   "source": [
    "# Lets pick some aribtrary values of w and b to test the loss function\n",
    "temp_w = [0.1, 0.1, 0.1, 0.1]\n",
    "temp_b = 0\n",
    "\n",
    "temp_loss = compute_loss(temp_w, temp_b, X, y)\n",
    "\n",
    "print(\"Your loss value:\")\n",
    "print(temp_loss)\n",
    "print()\n",
    "\n",
    "print(\"Correct loss value:\")\n",
    "print(0.04888214285714284)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Gradient (Graded)\n",
    "\n",
    "Now lets implement a function to calculate gradients for gradient descent. Inputs to the function are the input matrix <b>X</b>, weight vetor $\\vec{w}$, output vector $\\vec{y}$, weights vector $\\vec{w}$ and bias b:\n",
    "\n",
    "\n",
    "- Calculate $y' = W.x + b$.\n",
    "\n",
    "- Calculate $\\frac{dL}{db}$ as $db$\n",
    "\\begin{equation}\n",
    " \\frac{dL}{db} = \\Delta b =  \\frac{1}{m} \\sum_{i=1}^{m} (y'^{(i)}-y^{(i)})\n",
    "\\end{equation}\n",
    "\n",
    "- Calculate $\\frac{dL}{dw1}$ as $dw1$ \n",
    "\\begin{equation}\n",
    " \\frac{dL}{dw1} = \\Delta w = \\frac{1}{m} \\sum_{i=1}^{m} (y'^{(i)}-y^{(i)}).x_1^{(i)}\n",
    "\\end{equation}\n",
    "\n",
    "- Calculate $\\frac{dL}{dw2}$ as $dw2$ \n",
    "\\begin{equation}\n",
    " \\frac{dL}{dw2} = \\Delta w = \\frac{1}{m} \\sum_{i=1}^{m} (y'^{(i)}-y^{(i)}).x_2^{(i)}\n",
    "\\end{equation}\n",
    "\n",
    "- Calculate $\\frac{dL}{dw3}$ as $dw3$ \n",
    "\\begin{equation}\n",
    " \\frac{dL}{dw3} = \\Delta w = \\frac{1}{m} \\sum_{i=1}^{m} (y'^{(i)}-y^{(i)}).x_3^{(i)}\n",
    "\\end{equation}\n",
    "\n",
    "- Calculate $\\frac{dL}{dw4}$ as $dw4$ \n",
    "\\begin{equation}\n",
    " \\frac{dL}{dw4} = \\Delta w = \\frac{1}{m} \\sum_{i=1}^{m} (y'^{(i)}-y^{(i)}).x_4^{(i)}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient_4features(X, y, w, b):\n",
    "    \n",
    "    \n",
    "    m = X.shape[1]\n",
    "\n",
    "    # calculate predicted output y_primr\n",
    "\n",
    "    ########### WRITE YOU CODE BELOW ######\n",
    "    \n",
    "    dw1, dw2, dw3, dw4, db = 0, 0, 0, 0, 0\n",
    "    dw1,dw2,dw3,dw4 = 1/m * (w@X + b - y)@ X.T\n",
    "    db =  1/m * np.sum(((w@X + b) - y))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ########### END OF CODE ###############\n",
    "    \n",
    "    return dw1, dw2, dw3, dw4, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your graduent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Weights =  [-0.2498571428571426, -0.20571428571428543, -0.010857142857142711, -0.09553571428571422]\n",
      "Your bias =  -0.09357142857142849\n",
      "\n",
      "Expected Weights =  [-0.2498571428571428, -0.20571428571428563, -0.010857142857142899, -0.09553571428571425]\n",
      "Expected bias =  -0.09357142857142854\n"
     ]
    }
   ],
   "source": [
    "temp_w = [0.1, 0.1, 0.1, 0.1]\n",
    "temp_b = 0\n",
    "\n",
    "dw1, dw2, dw3, dw4, db = calculate_gradient_4features(X, y, temp_w, temp_b)\n",
    "\n",
    "print(\"Your Weights = \", [dw1, dw2, dw3, dw4])\n",
    "print(\"Your bias = \", db)\n",
    "\n",
    "print()\n",
    "print(\"Expected Weights = \", [-0.2498571428571428, -0.20571428571428563, -0.010857142857142899, -0.09553571428571425])\n",
    "print (\"Expected bias = \", -0.09357142857142854)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training your model (Graded)\n",
    "This will be your training loop. For every iteraion you will do the following:\n",
    "- Calcualte the $\\vec{dw} = [dw_1, dw_2, dw_3, dw_4]$ and $db$ using compute_gradient function.\n",
    "- Update the weights for $i=1, 2, 3, 4$ and bias $b$ using:\n",
    "\\begin{equation}\n",
    " w_i = w_i - (lr * dw_i)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    " b = b - (lr * db) \n",
    "\\end{equation}\n",
    "- Compute the cost for the updated weights and store it in the history for analysis later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regression(w, b, X, y, iterations, learning_rate, log_frequency):\n",
    "    \n",
    "    loss_history  = []\n",
    "    \n",
    "    # iterations defines how many times to execute the gradient descent step\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        ########### WRITE YOU CODE BELOW ######\n",
    "        \n",
    "        # computing the gradient\n",
    "        dw1,dw2,dw3,dw4,db = calculate_gradient_4features(X, y, w, b)\n",
    "\n",
    "        # updating the parameters\n",
    "        w -=learning_rate * np.array([dw1,dw2,dw3,dw4])\n",
    "        b -= learning_rate*db\n",
    "        \n",
    "        # calculate the cost for analysis later\n",
    "        loss = compute_loss(w, b, X, y)\n",
    "        \n",
    "        ########### END OF CODE ###############\n",
    "\n",
    "        # Periodically print the loss\n",
    "        if i % log_frequency == 0:\n",
    "            loss_history.append(loss)\n",
    "            print (\"Loss after iteration %i: %f\" %(i, loss))\n",
    "        \n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to use the training function with our housing price training data. To launch the training we need to decide the following parameters:\n",
    "\n",
    "- Learning rate\n",
    "- Numer of gradient descent steps to carry out\n",
    "- initialization for weights and bias\n",
    "\n",
    "In this first step we have already chosen these values for you. In the subsequent steps you will play with dfferent values of learning rate and number of iterations to see how that impacts the results.\n",
    "\n",
    "If your implementation is correct then using the training code below your final loss should be about 0.035"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 0.048639\n",
      "Loss after iteration 8: 0.047018\n",
      "Loss after iteration 16: 0.045813\n",
      "Loss after iteration 24: 0.044872\n",
      "Loss after iteration 32: 0.044097\n",
      "Loss after iteration 40: 0.043430\n",
      "Loss after iteration 48: 0.042833\n",
      "Loss after iteration 56: 0.042283\n",
      "Loss after iteration 64: 0.041765\n",
      "Loss after iteration 72: 0.041270\n",
      "Loss after iteration 80: 0.040792\n",
      "Loss after iteration 88: 0.040328\n",
      "Loss after iteration 96: 0.039875\n",
      "Loss after iteration 104: 0.039433\n",
      "Loss after iteration 112: 0.038999\n",
      "Loss after iteration 120: 0.038573\n",
      "Loss after iteration 128: 0.038156\n",
      "Loss after iteration 136: 0.037745\n",
      "Loss after iteration 144: 0.037342\n",
      "Loss after iteration 152: 0.036946\n",
      "Loss after iteration 160: 0.036557\n",
      "Loss after iteration 168: 0.036175\n",
      "Loss after iteration 176: 0.035799\n",
      "Loss after iteration 184: 0.035429\n",
      "Loss after iteration 192: 0.035066\n"
     ]
    }
   ],
   "source": [
    "iterations = 200 # training iterations\n",
    "learning_rate = 0.001 # learning rate\n",
    "\n",
    "# boolean for whether to print the logs\n",
    "print_cost = True\n",
    "\n",
    "# frequency with which to print the logs\n",
    "log_frequency = iterations/25\n",
    "\n",
    "# initializing the weights and bias (here we have chosen a known 'random' value so your results match ours)\n",
    "w = [0.1, 0.1, 0.1, 0.1]\n",
    "b = 0\n",
    "\n",
    "w, b, cost_history = train_regression(w, b, X, y, iterations, learning_rate, log_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the trained regression model (Graded)\n",
    "Here you test your model on the learned weights:\n",
    " \n",
    "- Define a prediction function to compute output vector y', given inputs vector w, bias b and input matrix X\n",
    "- Test how well the prediction matches the observed values of y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    \n",
    "    ########### WRITE YOU CODE BELOW ######\n",
    "    \n",
    "    y_prime = w@X + b\n",
    "    \n",
    "    ########### END OF CODE ###############\n",
    "    \n",
    "    return y_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed prices:                 [1.2  1.5  0.9  0.85 0.82 1.1  0.93]\n",
      "Prices Predicted by your model:  [1.17 1.27 0.8  0.94 0.51 1.36 1.02]\n",
      "Expected Prediction:             [1.17 1.27 0.8  0.94 0.51 1.36 1.02]\n"
     ]
    }
   ],
   "source": [
    "predicted_y = predict(w, b, X)\n",
    "\n",
    "# set how many decimal places to display\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(\"Observed prices:                \", y)\n",
    "print(\"Prices Predicted by your model: \", predicted_y)\n",
    "print(\"Expected Prediction:             [1.17 1.27 0.8  0.94 0.51 1.36 1.02]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve the training (Graded)\n",
    "In this section you will play with the learning rate and number of iterations to get to learn weights which have lower loss than our first training above.\n",
    "\n",
    "We expect you to be at least be able to achieve a loss of under 0.0005 (one of our solutions achieves about 0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 18.031124\n",
      "Loss after iteration 3600: 0.003308\n",
      "Loss after iteration 7200: 0.002386\n",
      "Loss after iteration 10800: 0.001769\n",
      "Loss after iteration 14400: 0.001327\n",
      "Loss after iteration 18000: 0.001011\n",
      "Loss after iteration 21600: 0.000785\n",
      "Loss after iteration 25200: 0.000623\n",
      "Loss after iteration 28800: 0.000507\n",
      "Loss after iteration 32400: 0.000424\n",
      "Loss after iteration 36000: 0.000364\n",
      "Loss after iteration 39600: 0.000322\n",
      "Loss after iteration 43200: 0.000291\n",
      "Loss after iteration 46800: 0.000269\n",
      "Loss after iteration 50400: 0.000254\n",
      "Loss after iteration 54000: 0.000242\n",
      "Loss after iteration 57600: 0.000234\n",
      "Loss after iteration 61200: 0.000229\n",
      "Loss after iteration 64800: 0.000225\n",
      "Loss after iteration 68400: 0.000222\n",
      "Loss after iteration 72000: 0.000220\n",
      "Loss after iteration 75600: 0.000218\n",
      "Loss after iteration 79200: 0.000217\n",
      "Loss after iteration 82800: 0.000216\n",
      "Loss after iteration 86400: 0.000216\n"
     ]
    }
   ],
   "source": [
    "########### WRITE YOU CODE BELOW ######\n",
    "\n",
    "iterations = 90000 # training iterations\n",
    "learning_rate = 0.01 # learning rate\n",
    "\n",
    "########### END OF CODE ###############\n",
    "\n",
    "# boolean for whether to print the logs\n",
    "print_cost = True\n",
    "\n",
    "# frequency with which to print the logs\n",
    "log_frequency = iterations/25\n",
    "\n",
    "# initializing the weights and bias\n",
    "w = np.random.rand(X.shape[0])\n",
    "b = 0\n",
    "\n",
    "w, b, cost_history = train_regression(w, b, X, y, iterations, learning_rate, log_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the improved model\n",
    "If you have achieved a loss under 0.0005 then your predicted outputs should now match observed values much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed prices:                 [1.2  1.5  0.9  0.85 0.82 1.1  0.93]\n",
      "Prices Predicted by your model:  [1.18 1.51 0.91 0.85 0.81 1.09 0.96]\n"
     ]
    }
   ],
   "source": [
    "predicted_y = predict(w, b, X)\n",
    "\n",
    "# set how many decimal places to display\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(\"Observed prices:                \", y)\n",
    "print(\"Prices Predicted by your model: \", predicted_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict some house prices!\n",
    "So far we have only tested how well our model is performing in predicting the prices of the houses that are already in our trainig set. Lets now use our model to predict prices for some houses that are not part of our training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed prices:  [1.7 0.7 0.9]\n",
      "Prices Predicted by your model:  [1.88 0.57 0.96]\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array([ [5,   1.7,  3.9 ],\n",
    "                    [5,   2,    2   ],\n",
    "                    [1.0, 2.5,  3.1 ],\n",
    "                    [1.0, 0.9,  0.75]])\n",
    "\n",
    "y_test = np.array([1.7, 0.7, 0.9])\n",
    "\n",
    "predicted_test_y = predict(w, b, X_test)\n",
    "\n",
    "# set how many decimal places ro print\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(\"Observed prices: \", y_test)\n",
    "print(\"Prices Predicted by your model: \", predicted_test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
